# chirpâ€‘transcriptâ€‘tool  

> **TL;DR** â€“ Turn one or many plainâ€‘text transcripts into naturalâ€‘sounding speech, clean the audio, and get wordâ€‘level timestamps â€“ all in a single commandâ€‘line run.  

The tool stitches together four Google Cloud services such as Vertexâ€¯AI Gemini, Cloudâ€¯Textâ€‘toâ€‘Speechâ€¯(Chirpâ€‘HDâ€‘3), Cloudâ€¯Storage (buckets) with two openâ€‘source libraries (ffmpeg, WhisperX) and a tiny CLI wrapper that handles flag parsing, cost reporting, and error handling.

---

**Disclaimer:**  
This code was writen as part of a learning project and is not a product software. By using it, modifying, or distributing this code, you acknowledge that you are solely responsible for all outcomes, consequences, and legal obligations arising from your actions. The author(s) and contributors provide no warranty or liability for any misuse, damages, or unintended effects resulting from the code.


## Table of Contents  

1. [Features](#features)  
2. [Presentation](#presentation)
3. [Architecture diagram](#architecture-diagram)  
4. [Prerequisites](#prerequisites)  
5. [Installation (UV)](#installation-uv)  
6. [Configuration (environment variables & `config.py`)](#configuration)  
7. [Commandâ€‘line interface](#cli)  
8. [Pipeline walkâ€‘through](#pipeline-walk-through)  
9. [Examples](#examples)  
10. [Cost reporting](#cost-reporting)  
11. [Testing & debugging](#testing--debugging)  
12. [Troubleshooting](#troubleshooting)  
13. [Documentation / Bibliography](#bibliography)  
14. [License](#license)  

---

## Features  

| âœ… | Description |
|----|-------------|
| **LLMâ€‘based transcript polishing** | Optional Geminiâ€‘Proâ€‘Previewâ€¯3 pass that injects punctuation, pauses, and other â€œhumanâ€‘likeâ€ cues according to the **Chirpâ€‘HDâ€‘3** documentation. |
| **Asynchronous TTS** | Sends asynchronously each (possibly tuned) transcript to **Google Cloud Textâ€‘toâ€‘Speech Longâ€‘Audio** (Chirpâ€‘HDâ€‘3) â†’ stores the result in a Cloudâ€¯Storage bucket â†’ downloads locally. |
| **Silence removal** | Oneâ€‘liner ffmpeg `silenceremove` filter trims long pauses, making the final audio sound more natural. |
| **Wordâ€‘level timestamps** | WhisperX (largeâ€‘v2) produces timestamps (start/end) times for every spoken word. |
| **Batch processing** | Accept a single file, a directory of `.txt` files, or a raw prompt. |
| **Cost awareness** | Tokenâ€‘based cost for Gemini, characterâ€‘based cost for Chirpâ€‘HDâ€‘3, printed perâ€‘file and overall. |
| **UVâ€‘based packaging** | All dependencies declared in `pyproject.toml`; install with the fast, modern `uv` tool. |
| **Extensible flag parser** | `tools/flags_parser.py` validates mutual exclusivity (`--prompt` vs `--fromâ€‘file/--fromâ€‘dir`). |

---

## Presentation

### Input text (from file)
```text
A lone astronomer set up a telescope on a desert ridge, hoping the clear night sky would reveal a secret. When a faint flicker appeared, the instrument captured a pattern that spelled out a longâ€‘lost melody encoded in starlight. The astronomer smiled, realizing the universe had been humming a song all along, waiting for someone to listen.
```

### Command
```bash
uv run main.py --from-file --no-bucket-preserve --verbose transcriptions/presentation_transcript.txt
```

### Log output
  
```
Initializing TTS client
Reading transcripts
Reading from file: transcriptions/presentation_transcript.txt
Loaded: transcriptions/presentation_transcript.txt
Reading Transcripts Done
Initializing LLM client
Setting LLM client for transcript tuning with model and getting response (gemini-3-pro-preview)
Got response - adding to list of responses

---Response---
A lone astronomer set up a telescope on a desert ridge, hoping the clear night sky would reveal a secret. [pause short] When a faint flicker appeared, the instrument captured a pattern that spelled out a longâ€‘lost melody encoded in starlight. [pause short] The astronomer smiled, realizing the universe had been humming a song all along... waiting for someone to listen.
---------


===OVERALL COST===
Prompt tokens: 2007.0 --- Cost: 0.00802800 $
Response tokens: 75.0 --- Cost: 0.00060000 $
Summary: 2082.0 tokens --- 0.00862800 $
===$$$===

Setting TTS client with model and storage client (Chirp - Sadaltager) and sending request.
Requesting Audio content to bucket: gs://bucket1/response_audio/presentation_transcript_20251219_134905.wav
Audio synthesized to GCS. Downloading: gs://bucket1/response_audio/presentation_transcript_20251219_134905.wav
Blob deleted: response_audio/presentation_transcript_20251219_134905.wav
Finished: presentation_transcript_20251219_134905.wav

===OVERALL COST===
Tokens: 370 --- Cost: 0.011100 $
===$$$===

Running ffmpeg remove silence
Removed silence and saved file to: edited_audio/presentation_transcript_20251219_134905.wav
Done cut silence. Files saved to directory: edited_audio
Loading WhisperX on cuda, batch_size = 8, compute_type = float16
Loading allign model
Transcribing audio
Cleaning memory - transcribe
Generating timestamps
Timestamps for file edited_audio/presentation_transcript_20251219_134905.wav generated.
Cleaning
Transcribing done file saved to timestamped_transcriptions/output.txt
```

### Tuned transcript (by Gemini-3-Pro-Preview)

```text
A lone astronomer set up a telescope on a desert ridge, hoping the clear night sky would reveal a secret. [pause short] When a faint flicker appeared, the instrument captured a pattern that spelled out a longâ€‘lost melody encoded in starlight. [pause short] The astronomer smiled, realizing the universe had been humming a song all along... waiting for someone to listen.
```

### Audio from TTS model (Chirp - voice Sadaltager)

ğŸµ [Click here to listen to the audio player](https://archbober.github.io/chirp-transcript-tool/audio_response.html)

### Audio after removing silence

ğŸµ [Click here to listen to the audio player](https://archbober.github.io/chirp-transcript-tool/audio_edit.html)

### Transcript with timestamps from file (per word for now in List[dict[str,str]] form)
```
[{'word': 'A', 'start': np.float64(0.031), 'end': np.float64(0.613)}, {'word': 'lone', 'ord': 'pattern', 'start': np.float64(9.301), 'end': np.float64(9.642)}, ... -> ... {'word': 'listen.', 'start': np.float64(18.55), 'end': np.float64(19.052)}]
```

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input (txt / prompt)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   read_transcripts  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Is LLM tuning enabled? (â€“noâ€‘tuning) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚
   Yes  â–¼                       â–¼   No
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚      llm()          â”‚   â”‚ (skip tuning step)  â”‚
 â”‚(Geminiâ€‘Proâ€‘Preview3)â”‚   â”‚                     â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                         â”‚
         â–¼                         â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
 â”‚  Async  tts_chirp() â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚ (Google TTS â€“ Chirp)â”‚   
 â”‚  â†’ upload to GCS    â”‚   
 â”‚      (bucket)       â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
         â”‚                         
         â–¼                         
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   
 â”‚       Async         â”‚
 â”‚ Download from GCS   â”‚   
 â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 
         â”‚                         
         â–¼                         
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
 â”‚   ffmpeg_cutter()   â”‚  
 â”‚ (remove long pauses)â”‚   
 â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   
         â”‚     
         â–¼     
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ stt_timestamps()    â”‚   
 â”‚     WhisperX        â”‚
 â”‚    wordâ€‘level       â”‚
 â”‚    timestamps       â”‚   
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   
 ```

 ## Prerequisites  

| Item | Minimum version / notes |
|------|------------------------|
| **Python** | 3.13 (declared in `pyproject.toml`) |
| **UV** | `uv >=0.4` â€“ the recommended installer for the project |
| **ffmpeg** | Must be on the system `$PATH` (`ffmpeg -version` works) |
| **GPU (optional)** | CUDAâ€‘enabled GPU for WhisperX speedâ€‘up; otherwise CPU fallback works (slower) |
| **Google Cloud project** | Vertexâ€¯AI, Cloudâ€¯Storage, Cloudâ€¯Textâ€‘toâ€‘Speech APIs enabled |
| **Serviceâ€‘account JSON** | With roles `aiplatform.user`, `storage.objectAdmin`, `texttospeech.admin` |

---

## Installation (UV)  

```bash
# 1ï¸âƒ£ Clone the repo
git clone https://github.com/ArchBober/chirp-transcript-tool.git
cd chirp-transcript-tool

# 2ï¸âƒ£ Install UV (if you donâ€™t have it yet)
curl -LsSf https://astral.sh/uv/install.sh | sh

# 3ï¸âƒ£ Create an isolated environment and install deps
uv venv .venv               # creates .venv/
source .venv/bin/activate   # Windows: .venv\Scripts\activate

# 4ï¸âƒ£ Install the package (reads pyproject.toml)
uv sync                     # resolves & installs all dependencies
```

> **Why UV?**  
> UV resolves the dependency graph **much faster** than `pip` and writes a lockfile (`uv.lock`) that guarantees reproducible builds. It also respects the `[project]` table from `pyproject.toml`, so you donâ€™t need a separate `requirements.txt`.

## Configuration  

### Environment variables (`.env`)  

Create a file named `.env` in the repository root:

```dotenv
VERTEX_AI_API_KEY=<yourâ€‘vertexâ€‘apiâ€‘key>
SECRET_JSON_FILEPATH=/absolute/path/to/service-account.json
BUCKET_NAME=<yourâ€‘gcsâ€‘bucket>
```

The script loads these with `python-dotenv`.  

### `config.py` (already shipped)  

| Constant | Meaning |
|----------|---------|
| `TTS_TEXT_FILE` | Default transcript used when no file flag is supplied. |
| `OUTPUT_AUDIO_DIR` / `EDITED_AUDIO_DIR` | Where raw and cleaned audio are stored locally. |
| `SPEAKING_RATE` | Default 1.0Ã— (range 0.5â€‘2.0). |
| `TTS_VOICE` | `"Sadaltager"` â€“ a Chirpâ€‘HDâ€‘3 voice. |
| `LANGUAGE` | `"enâ€‘US"` â€“ language code for TTS. |
| `LLM_MODEL` | `"gemini-3-pro-preview"` â€“ the Gemini model used for tuning. (can be changed to cheaper one model) |
| `LLM_CHIRP_PROMPT` | Prompt that injects the **Chirpâ€‘HDâ€‘3** scripting guidelines (see `descriptions/prompt_chirp_doc.py` and `config.py`). |
| `LLM_INPUT_TOKEN_PRICE`, `LLM_OUTPUT_TOKEN_PRICE`, `TTS_CHIRP_TOKEN_PRICE` | Pricing constants (USD perâ€¯M tokens/characters). Update them if Google changes its rates. |

### Prompt documentation (`descriptions/prompt_chirp_doc.py` && `config.py`)  

Contains the full â€œScripting and prompting tipsâ€ you posted â€“ the LLM uses this to add naturalâ€‘speech cues without altering the original meaning.

## Commandâ€‘line interface  

Run the entry point with `uv run main.py`. Flags are defined in `tools/flags_parser.py`.

| Flag | Alias | Description |
|------|-------|-------------|
| `--verbose` | â€“ | Print detailed progress and cost info. |
| `--cost-single` | â€“ | Show perâ€‘file TTS cost. |
| `--prompt` | â€“ | First positional argument is treated as a raw prompt (no file reading). |
| `--noâ€‘tuning` | â€“ | Skip the Gemini LLM step; feed transcripts straight to TTS. |
| `--noâ€‘bucketâ€‘preserve` | â€“ | Delete the temporary audio object from Cloudâ€¯Storage after download. |
| `--fromâ€‘file` | â€“ | Load a **single** transcript file (default path or `TTS_TEXT_FILE`). |
| `--fromâ€‘dir` | â€“ | Load **all** `.txt` files from the supplied directory. |
| `--help` | â€“ | Show the help description (`HELP_DESCRIPTION`). |
| `positional` | â€“ | Remaining arguments â€“ either the prompt text (`--prompt`) or the path(s) for `--fromâ€‘file/--fromâ€‘dir`. |

**Mutual exclusions** (enforced by the parser):

* `--prompt` cannot be combined with `--fromâ€‘file` or `--fromâ€‘dir`.  
* `--fromâ€‘file` and `--fromâ€‘dir` cannot be used together.

## Pipeline walkâ€‘through  

1. **Parse flags** â€“ `tools/flags_parser.parse_flags()` validates the command line.  
2. **Read transcripts** â€“ `tools/read_transcripts.py` loads one file or an entire directory into a `Dict[str,str]`.  
3. **LLM tuning (optional)** â€“ `model_tools/llm.py` sends each transcript to Gemini (`genai.Client`). The system prompt (`LLM_CHIRP_PROMPT`) forces the model to only add punctuation/pauses/IPA tags described in the Chirp documentation.  
4. **Asynchronous TTS** â€“ `model_tools/tts_chirp.py`  
   * Calls `texttospeech.TextToSpeechLongAudioSynthesizeAsyncClient` with the refined text.  
   * Stores the generated WAV in the bucket (`gs://<BUCKET>/<OUTPUT_AUDIO_DIR>/â€¦`).  
   * Downloads the file locally, optionally deleting the bucket object (`--noâ€‘bucketâ€‘preserve`).  
5. **Silence removal** â€“ `tools/ffmpeg_cutter.py` runs `ffmpeg -af silenceremove=stop_periods=-1:stop_duration=0.2:stop_threshold=-40dB` removing silence longer than 0.2s and quiter than -40 dB (if those parameters are not changed by user).  
6. **Timestamp extraction** â€“ `model_tools/stt.py` loads WhisperX (largeâ€‘v2) on the cleaned audio, aligns wordâ€‘level timestamps, and writes them to `timestamped_transcriptions/output.txt`. Model can be changed. Also this library is very problematic so its worth to look on whisperx issue page if there will be some problems with environment/downloading etc.  

All heavyâ€‘weight I/O (bucket upload/download, TTS requests) is performed **asynchronously** with `asyncio.gather`, dramatically reducing total runtime for multiâ€‘file batches (if not async it would do 1 file at a time which is quite long especially if many heavy inputs are delivered). That means max time waiting for TTS execution = largest file. 

## Examples  

### 1ï¸âƒ£ Prompt â†’ LLM â†’ TTS â†’ Clean â†’ Timestamps  

```bash
export VERTEX_AI_API_KEY=abcd123...890xyz
export SECRET_JSON_FILEPATH=$HOME/gcloud/key.json
export BUCKET_NAME=chirp-audio-bucket

uv run main.py \
    --prompt \
    --verbose \
    "What is lorem ipsum."

```

**What you get**

* `response_audio/name_<timestamp>.wav` â€“ raw Chirp synthesis.  
* `edited_audio/name_<timpestamp>.wav` â€“ silenceâ€‘removed version.  
* `timestamped_transcriptions/output.txt` â€“ JSONâ€‘like list of `{word,start,end}`. (for now later add more options/ better option to save timestamps)

### 2ï¸âƒ£ Batch directory, skip LLM, keep bucket objects  

```bash
uv run main.py \
    --from-dir \
    --no-tuning \
    --cost-single ./my_transcripts

```

Processes every `*.txt` under `./my_transcripts`, prints perâ€‘file TTS cost, and leaves the intermediate objects in the bucket (useful for later inspection).

### 3ï¸âƒ£ Using a file not preserving copy on bucket and disabling tuning (skip LLM)  

```bash
uv run main.py \
    --from-file \
    --no-bucket-preserve \
    --no-tuning ./script.txt
```

## Cost reporting  

When `--verbose` (or `--cost-single`) is active the script prints three sections:

| Section | What is shown |
|---------|---------------|
| **LLM cost** | `prompt tokens Ã— LLM_INPUT_TOKEN_PRICE` and `response tokens Ã— LLM_OUTPUT_TOKEN_PRICE`. |
| **TTS cost** | `character count Ã— TTS_CHIRP_TOKEN_PRICE`. |
| **Overall** | Sum of the above for the whole run. |

The numbers are **USD** (based on the constants in `config.py`). Adjust the constants if Google updates its pricing.

## Troubleshooting  

| Symptom | Likely cause | Fix |
|---------|--------------|-----|
| `ffmpeg: command not found` | ffmpeg not installed or not on `$PATH`. | Install via `apt-get install ffmpeg`, `brew install ffmpeg`, or download from https://ffmpeg.org/. |
| `google.api_core.exceptions.PermissionDenied` | Serviceâ€‘account lacks required IAM roles. | Grant `roles/aiplatform.user`, `roles/texttospeech.admin`, `roles/storage.objectAdmin` to the service account. |
| `Operation timed out` while waiting for TTS | Very long transcript (>â€¯50â€¯k characters) or network latency. | Split the transcript into smaller chunks (â‰¤â€¯5000â€¯chars) before calling `tts_chirp()` or set higher timeout in tts_chrip.py ex. -> await operation.result(timeout=600). |
| WhisperX runs on CPU and is extremely slow | No CUDA device detected. | Install CUDA drivers and the matching `torch` wheel ex. -> (`uv add torch --extra-index-url https://download.pytorch.org/whl/cu121`). But for sanity its worth checking whisperx repository for solutions. |
| Empty `output.txt` after STT | Audio file never downloaded or corrupted. | Verify that `ffmpeg_cutter` produced a nonâ€‘zeroâ€‘size WAV; reâ€‘run with `--verbose` to see the download URI. |
| `--prompt` and `--from-dir` used together â†’ script exits | Parser correctly aborts. | Choose one mode only â€“ either a raw prompt or a directory of files. |

## Bibliography  

| # | Resource | Link |
|---|----------|------|
| 1 | **Google Vertex AI â€“ Gemini Pro Preview** (LLM used for transcript polishing) | <https://docs.cloud.google.com/vertex-ai/docs/start/introduction-unified-platform> |
| 2 | **Google Cloud Textâ€‘toâ€‘Speech â€“ Chirpâ€¯HDâ€¯3** (highâ€‘quality neural voice) | <https://docs.cloud.google.com/text-to-speech/docs/chirp3-hd> |
| 3 | **Google Cloud Storage** (temporary bucket for longâ€‘audio synthesis) | <https://docs.cloud.google.com/storage/docs> |
| 4 | **ffmpeg â€“ silenceremove filter** (removing long pauses) | <https://ffmpeg.org/ffmpeg-filters.html#silenceremove> |
| 5 | **WhisperX â€“ wordâ€‘level timestamps** (openâ€‘source speechâ€‘toâ€‘text) | <https://github.com/m-bain/whisperX> |
| 6 | **UV â€“ a fast Python package manager** | <https://github.com/astral-sh/uv> |
| 7 | **pythonâ€‘dotenv** (loading `.env` files) | <https://pypi.org/project/python-dotenv/> |
| 8 | **Googleâ€‘GenAI Python SDK** (client for Gemini) | <https://pypi.org/project/google-genai/> |
| 9 | **Googleâ€‘Cloudâ€‘Textâ€‘toâ€‘Speech Python client** | <https://pypi.org/project/google-cloud-texttospeech/> |
|10| **Googleâ€‘Cloudâ€‘Storage Python client** | <https://pypi.org/project/google-cloud-storage/> |
|11| **OpenAIâ€‘Whisper** (fallback STT model used by WhisperX) | <https://github.com/openai/whisper> |
|12| **Prompt engineering guide for Chirp** (the large block you supplied) | *Embedded in `descriptions/prompt_chirp_doc.py`* |

## License  

This project is released under the **MIT License**. See the `LICENSE` file for full terms.

---  

*Enjoy turning text into naturalâ€‘sounding audio!* ğŸš€